# Documenta√ß√£o T√©cnica: Sistema de Detec√ß√£o de Envenenamento

## Vis√£o Geral do Projeto

Este projeto implementa um **Sistema de Detec√ß√£o de Envenenamento de Modelos** em cen√°rios de Aprendizado Federado. O sistema simula ataques de data poisoning em clientes maliciosos e desenvolve m√©todos para detectar e mitigar esses ataques no servidor central.

### Caracter√≠sticas Principais
- **Simula√ß√£o de Aprendizado Federado**: M√∫ltiplos clientes treinam localmente
- **Data Poisoning**: Manipula√ß√£o de labels (7‚Üí1) para confundir o modelo
- **An√°lise de Performance**: M√©tricas de acur√°cia e matriz de confus√£o
- **Explicabilidade**: Integra√ß√£o com LIME para interpreta√ß√£o de decis√µes
- **Detec√ß√£o de Anomalias**: Identifica√ß√£o de comportamentos suspeitos em clientes

---

## Sum√°rio

### 1. Arquitetura do Sistema
- [1.1 Estrutura Geral do Projeto](#11-estrutura-geral-do-projeto)
- [1.2 Fluxo de Aprendizado Federado](#12-fluxo-de-aprendizado-federado)
- [1.3 Sistema de Coordena√ß√£o (controler.py)](#13-sistema-de-coordena√ß√£o-controlerpy)

### 2. Arquitetura da Rede Neural
- [2.1 Classe Net (modelNet.py)](#21-classe-net-modelnetpy)
  - [2.1.1 Inicializa√ß√£o da Rede](#211-inicializa√ß√£o-da-rede)
  - [2.1.2 M√©todo forward()](#212-m√©todo-forward)
  - [2.1.3 M√©todo predict() para LIME](#213-m√©todo-predict-para-lime)
- [2.2 Fluxo de Dados Completo](#22-fluxo-de-dados-completo)

### 3. M√≥dulos de Treinamento
- [3.1 Treinamento de Clientes (clients/)](#31-treinamento-de-clientes-clients)
- [3.2 Treinamento Central (central/)](#32-treinamento-central-central)
- [3.3 Configura√ß√µes do Sistema (sysvars.py)](#33-configura√ß√µes-do-sistema-sysvars)

### 4. An√°lises e Detec√ß√£o
- [4.1 Matriz de Confus√£o (get_confusion_map.py)](#41-matriz-de-confus√£o-get_confusion_mappy)
- [4.2 An√°lise de Acur√°cia (view_acc.py)](#42-an√°lise-de-acur√°cia-view_accpy)
- [4.3 Manipula√ß√£o de Dados (Data Poisoning)](#43-manipula√ß√£o-de-dados-data-poisoning)

### 5. Conceitos Fundamentais do PyTorch
- [5.1 Camadas Convolucionais (Conv2d)](#51-camadas-convolucionais-conv2d)
- [5.2 Regulariza√ß√£o com Dropout](#52-regulariza√ß√£o-com-dropout)
- [5.3 Camadas Lineares (Linear)](#53-camadas-lineares-linear)
- [5.4 Fun√ß√µes de Ativa√ß√£o](#54-fun√ß√µes-de-ativa√ß√£o)
  - [5.4.1 ReLU](#541-relu)
  - [5.4.2 Softmax e LogSoftmax](#542-softmax-e-logsoftmax)
- [5.5 Max Pooling](#55-max-pooling)
- [5.6 Normaliza√ß√£o de Dados](#56-normaliza√ß√£o-de-dados)

---

## 1. Arquitetura do Sistema

### 1.1 Estrutura Geral do Projeto

O projeto segue uma arquitetura modular inspirada em sistemas de aprendizado federado reais:

```
poison-detector/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py          # Ponto de entrada principal
‚îÇ   ‚îú‚îÄ‚îÄ controler.py          # Coordena√ß√£o e orquestra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ modelNet.py           # Defini√ß√£o da CNN
‚îÇ   ‚îú‚îÄ‚îÄ sysvars.py           # Configura√ß√µes globais
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ clients/             # Simula√ß√£o de clientes federados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py         # Treinamento local
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_mnist.py   # Fun√ß√µes espec√≠ficas MNIST
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/          # Modelos treinados localmente
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ central/             # Servidor central federado
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py         # Agrega√ß√£o e treinamento central
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analyses/            # Ferramentas de an√°lise
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_confusion_map.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ view_acc.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ graphics/        # Visualiza√ß√µes
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ data/               # Datasets (MNIST original e envenenado)
‚îÇ       ‚îî‚îÄ‚îÄ MNIST/
‚îÇ           ‚îî‚îÄ‚îÄ raw/        # Dados bin√°rios MNIST
```

### 1.2 Fluxo de Aprendizado Federado

**Pipeline completo do sistema:**

```mermaid
graph TB
    A[Dashboard] --> B[Controler]
    B --> C[Manipula√ß√£o de Dados]
    C --> D[Dataset Envenenado]
    C --> E[Dataset Original]
    
    D --> F[Cliente Malicioso]
    E --> G[Cliente Benigno]
    
    F --> H[Modelo Local 1]
    G --> I[Modelo Local 2]
    
    H --> J[Servidor Central]
    I --> J
    
    J --> K[Agrega√ß√£o de Pesos]
    K --> L[Modelo Global]
    
    L --> M[An√°lise de Performance]
    M --> N[Detec√ß√£o de Anomalias]
```

**Fases operacionais:**

1. **Prepara√ß√£o**: Cria√ß√£o de datasets envenenados
2. **Treinamento Distribu√≠do**: Clientes treinam independentemente
3. **Agrega√ß√£o**: Servidor combina modelos locais
4. **An√°lise**: Detec√ß√£o de comportamentos an√¥malos
5. **Mitiga√ß√£o**: Exclus√£o de clientes suspeitos

### 1.3 Sistema de Coordena√ß√£o (controler.py)

O `controler.py` atua como **orquestrador central** do sistema, gerenciando todas as opera√ß√µes principais.

#### Principais Funcionalidades

**1. Gerenciamento de Treinamento**
```python
def post_client_train():
    """Simula sess√µes de treinamento local (cliente)"""
    # Cliente benigno - dataset original
    client_train(epochs=100, data_path=svar.PATH_BASE_DATASET.value)
    
    # Cliente malicioso - dataset envenenado  
    client_train(epochs=100, data_path="./datasets/poisoned_data_set_1/")
```

**2. Agrega√ß√£o Federada**
```python
def post_central_train(selected_indice_models: list = [-1]):
    """Simula agrega√ß√£o no servidor central"""
    # Carrega pesos dos clientes selecionados
    new_model = get_weights(isCentral=False, selected_indice_models=selected_indice_models)
    
    # Realiza treinamento central com pesos agregados
    central_train(new_model=new_model_dict)
```

**3. Sistema de Carregamento de Modelos**
```python
def get_weights(isCentral=True, selected_indice_models: list = []):
    """Carrega state_dicts de modelos salvos do disco"""
    
    # Estrat√©gias de sele√ß√£o:
    # [] - todos os modelos
    # [-1] - apenas o mais recente  
    # [1,3,5] - modelos espec√≠ficos por √≠ndice
```

**4. Data Poisoning**
```python
def manipule_data():
    """Implementa ataque de envenenamento de dados"""
    # Carrega MNIST original
    mnist_trainset = datasets.MNIST(...)
    
    # Ataque: muda todas as labels 7 ‚Üí 1
    for i in range(len(mnist_trainset)):
        if mnist_trainset.targets[i] == 7:
            mnist_trainset.targets[i] = 1
    
    # Salva dataset modificado
    torch.save((train_data, train_targets), 
              './datasets/poisoned_data_set_1/training.pt')
```

**5. Pipeline de An√°lises**
```python
def get_analyses():
    """Executa an√°lises de performance e detecta anomalias"""
    # Carrega modelos espec√≠ficos (benigno vs malicioso)
    models = get_weights(isCentral=False, selected_indice_models=[4, 5])
    
    # Compara acur√°cias
    acc_benign = get_accuracy(models["model_4.pt"])  
    acc_malign = get_accuracy(models["model_5.pt"])
    
    # Detecta degrada√ß√£o causada por poisoning
    print(f"Acur√°cia Benigno: {acc_benign}")
    print(f"Acur√°cia Malicioso: {acc_malign}")
```

---

## 2. Arquitetura da Rede Neural

### 2.1 Classe Net (modelNet.py)

---

## 1. Arquitetura da Rede Neural

### 2.1 Classe Net (modelNet.py)

A classe `Net` implementa uma Rede Neural Convolucional (CNN) para classifica√ß√£o de d√≠gitos MNIST. Esta arquitetura segue o padr√£o cl√°ssico de extra√ß√£o hier√°rquica de caracter√≠sticas seguida por classifica√ß√£o.

**Contexto no Sistema**: A mesma arquitetura √© usada tanto por clientes benignos quanto maliciosos, permitindo comparar como o data poisoning afeta modelos id√™nticos.

**Estrutura Geral:**
```
Entrada (28x28) ‚Üí Conv1 ‚Üí ReLU ‚Üí MaxPool ‚Üí Conv2 ‚Üí Dropout ‚Üí ReLU ‚Üí MaxPool ‚Üí Flatten ‚Üí FC1 ‚Üí ReLU ‚Üí Dropout ‚Üí FC2 ‚Üí LogSoftmax
```

#### 2.1.1 Inicializa√ß√£o da Rede

```python
def __init__(self):
    super(Net, self).__init__()
    
    # Camadas convolucionais
    self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)
    self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)
    self.conv2_drop = nn.Dropout2d(p=0.5)
    
    # Camadas totalmente conectadas
    self.fc1 = nn.Linear(in_features=320, out_features=50)
    self.fc2 = nn.Linear(in_features=50, out_features=10)
```

**Componentes:**

1. **conv1**: Primeira [camada convolucional](#21-camadas-convolucionais-conv2d)
   - Detecta caracter√≠sticas b√°sicas (bordas, linhas)
   - Transforma: (1,28,28) ‚Üí (10,24,24)

2. **conv2**: Segunda [camada convolucional](#21-camadas-convolucionais-conv2d)
   - Detecta padr√µes complexos (formas, curvas)
   - Transforma: (10,12,12) ‚Üí (20,8,8)

3. **conv2_drop**: [Dropout 2D](#22-regulariza√ß√£o-com-dropout) para regulariza√ß√£o
   - Previne overfitting zerando mapas aleatoriamente

4. **fc1**: Primeira [camada linear](#23-camadas-lineares-linear)
   - Integra caracter√≠sticas: 320 ‚Üí 50 neur√¥nios
   - **C√°lculo do 320**: 20 canais √ó 4√ó4 pixels = 320

5. **fc2**: [Camada linear](#23-camadas-lineares-linear) de classifica√ß√£o
   - Produz sa√≠das finais: 50 ‚Üí 10 classes (d√≠gitos 0-9)

#### 1.1.2 M√©todo forward()

Define como os dados fluem atrav√©s da rede durante a infer√™ncia:

```python
def forward(self, x):
    # Primeiro bloco convolucional
    x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))
    
    # Segundo bloco convolucional com regulariza√ß√£o
    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), kernel_size=2))
    
    # Achatar para camadas densas
    x = x.view(-1, 320)
    
    # Camadas totalmente conectadas
    x = F.relu(self.fc1(x))
    x = F.dropout(x, training=self.training)
    x = self.fc2(x)
    
    return F.log_softmax(x, dim=1)
```

**Transforma√ß√µes por etapa:**
1. **Entrada**: (batch_size, 1, 28, 28)
2. **Ap√≥s conv1+pool**: (batch_size, 10, 12, 12)
3. **Ap√≥s conv2+pool**: (batch_size, 20, 4, 4)
4. **Ap√≥s flatten**: (batch_size, 320)
5. **Ap√≥s fc1**: (batch_size, 50)
6. **Sa√≠da final**: (batch_size, 10)

#### 1.1.3 M√©todo predict() para LIME

Interface especial para integra√ß√£o com LIME (Local Interpretable Model-agnostic Explanations):

```python
def predict(self, images):
    # Pr√©-processamento autom√°tico
    transform = transforms.Compose([...])
    
    # Convers√£o e processamento em lote
    batch = torch.stack([transform(img) for img in images])
    
    # Infer√™ncia sem gradientes
    with torch.no_grad():
        log_probs = self.forward(batch)
        probs = F.softmax(log_probs, dim=1)
    
    return probs.cpu().numpy()
```

**Caracter√≠sticas especiais:**
- Aceita lista de imagens numpy
- Aplica [normaliza√ß√£o MNIST](#26-normaliza√ß√£o-de-dados) automaticamente
- Retorna probabilidades (n√£o log-probabilidades)
- Otimizado para an√°lise de explicabilidade

### 1.2 Fluxo de Dados Completo

**Diagrama de transforma√ß√µes:**

```
Input: Imagem 28√ó28 grayscale
    ‚Üì
[Conv1: kernel=5√ó5, filtros=10]
    ‚Üì Tamanho: 28-5+1 = 24√ó24
[ReLU + MaxPool 2√ó2]
    ‚Üì Tamanho: 24√∑2 = 12√ó12
[Conv2: kernel=5√ó5, filtros=20]
    ‚Üì Tamanho: 12-5+1 = 8√ó8
[Dropout2D + ReLU + MaxPool 2√ó2]
    ‚Üì Tamanho: 8√∑2 = 4√ó4
[Flatten: 20√ó4√ó4 = 320]
    ‚Üì
[FC1: 320‚Üí50 + ReLU + Dropout]
    ‚Üì
[FC2: 50‚Üí10 + LogSoftmax]
    ‚Üì
Output: Log-probabilidades para 10 classes
```

---

## 3. M√≥dulos de Treinamento

### 3.1 Treinamento de Clientes (clients/)

O m√≥dulo `clients/` simula o comportamento de dispositivos participantes em aprendizado federado. Cada cliente treina independentemente com seus dados locais.

#### 3.1.1 Fun√ß√£o post_train() (clients/train.py)

Interface flex√≠vel para treinamento local com m√∫ltiplas configura√ß√µes:

```python
def post_train(**kwargs):
    """Treina modelo MNIST com par√¢metros customiz√°veis"""
    args = {
        "batch_size": kwargs.get("batch_size", 64),
        "epochs": kwargs.get("epochs", 20),
        "lr": kwargs.get("lr", 0.01),
        "momentum": kwargs.get("momentum", 0.5),
        "model_static_dict": kwargs.get("model_static_dict", {}),  # Fine-tuning
        "load_data": kwargs.get("load_data", False),
        "data_path": kwargs.get("data_path", svar.PATH_BASE_DATASET.value)
    }
```

**Caracter√≠sticas principais:**

- **Flexibilidade**: Aceita qualquer combina√ß√£o de hiperpar√¢metros
- **Fine-tuning**: Pode carregar pesos pr√©-treinados via `model_static_dict`
- **Multi-dataset**: Suporta datasets originais e envenenados
- **Serializa√ß√£o segura**: Configura√ß√£o especial do PyTorch para classes customizadas

**Exemplo de uso no contexto federado:**
```python
# Cliente benigno
client_train(epochs=100, data_path="./datasets/base_data_set/")

# Cliente malicioso  
client_train(epochs=100, data_path="./datasets/poisoned_data_set_1/")
```

#### 3.1.2 Gerenciamento de Dispositivos

O sistema adapta automaticamente recursos baseado na disponibilidade de GPU:

```python
device = svar.DEFAULT_DEVICE.value  # "cuda" por padr√£o
kwargs = {'num_workers': 8, 'pin_memory': True} if device == 'cuda' else {}
```

**Otimiza√ß√µes CUDA:**
- `num_workers=8`: Paraleliza√ß√£o no carregamento de dados
- `pin_memory=True`: Transfer√™ncia mais r√°pida CPU‚ÜíGPU
- `model.to(device)`: Migra√ß√£o autom√°tica do modelo

### 3.2 Treinamento Central (central/)

O m√≥dulo `central/` implementa o servidor de agrega√ß√£o federada, combinando conhecimento de m√∫ltiplos clientes.

**Processo de agrega√ß√£o t√≠pico:**
1. **Coleta**: Recebe pesos de clientes selecionados
2. **Agrega√ß√£o**: Combina pesos (m√©dia ponderada, FedAvg, etc.)
3. **Refinamento**: Treina modelo global com dados centrais
4. **Distribui√ß√£o**: Envia modelo atualizado de volta aos clientes

**Estrat√©gias de sele√ß√£o de clientes:**
- **Todos**: `selected_indice_models = []`
- **Mais recente**: `selected_indice_models = [-1]`
- **Espec√≠ficos**: `selected_indice_models = [1, 3, 5]`

### 3.3 Configura√ß√µes do Sistema (sysvars.py)

Centraliza√ß√£o de configura√ß√µes cr√≠ticas do sistema usando Enum para type safety:

```python
class SysVars(Enum):
    DEFAULT_DEVICE = "cuda"                           # GPU principal
    PATH_BASE_DATASET = "./datasets/base_data_set/"   # MNIST original
    PATH_CLIENT_MODELS = "./clients/models/"          # Modelos locais
    PATH_CENTRAL_MODELS = "./central/models/"         # Modelos agregados
    PATH_ANALYSES_GRAPHICS = "./analyses/graphics/"   # Visualiza√ß√µes
    PATH_ANALYSES_CVS = "./analyses/csv/"            # Dados tabulares
```

**Vantagens da abordagem Enum:**
- **Type Safety**: Previne erros de string
- **Centraliza√ß√£o**: Mudan√ßas propagam automaticamente
- **IDE Support**: Autocompletar e refactoring
- **Constantes**: Valores imut√°veis por design

---

## 4. An√°lises e Detec√ß√£o

### 4.1 Matriz de Confus√£o (get_confusion_map.py)

Ferramenta essencial para detectar padr√µes an√¥malos causados por data poisoning.

#### 4.1.1 Gera√ß√£o da Matriz

```python
def get_confusion_map(state_dict, model_id, data_path):
    """Gera matriz de confus√£o detalhada para um modelo"""
    
    # Estrutura de dados para contagem
    predicts = {
        0: {0: 0, 1: 0, ..., 9: 0},  # Label real 0
        1: {0: 0, 1: 0, ..., 9: 0},  # Label real 1  
        ...
        9: {0: 0, 1: 0, ..., 9: 0}   # Label real 9
    }
    
    # Avalia√ß√£o modelo por modelo
    for data, target in test_loader:
        output = model(data)
        pred = output.argmax(dim=1, keepdim=True)
        predicts[target.item()][pred.item()] += 1
```

#### 4.1.2 Detec√ß√£o de Anomalias

**Padr√µes suspeitos indicativos de poisoning:**

1. **Off-diagonal elevado**: Confus√£o sistem√°tica entre classes espec√≠ficas
2. **Assimetria**: 7‚Üí1 mas n√£o 1‚Üí7 (indica ataque direcionado)
3. **Performance seletiva**: Alta acur√°cia em algumas classes, baixa em outras

**Exemplo de matriz suspeita:**
```
     Predito
Real  0  1  2  3  4  5  6  7  8  9
  0  95  2  1  0  1  0  1  0  0  0  ‚Üê Normal
  1   1 94  2  1  1  0  0  0  1  0  ‚Üê Normal
  7   0 89  0  0  0  0  0  8  1  2  ‚Üê SUSPEITO! 89% dos 7s preditos como 1
```

### 4.2 An√°lise de Acur√°cia (view_acc.py)

Sistema de m√©tricas quantitativas para compara√ß√£o de modelos e detec√ß√£o de degrada√ß√£o.

**M√©tricas implementadas:**
- **Acur√°cia geral**: Performance global do modelo
- **Acur√°cia por classe**: Identifica classes espec√≠ficas afetadas
- **Degrada√ß√£o relativa**: Compara√ß√£o com baseline benigno

**Uso na detec√ß√£o:**
```python
# Compara√ß√£o direta entre modelos
acc_benign = get_accuracy(models["model_4.pt"])   # Cliente benigno
acc_malign = get_accuracy(models["model_5.pt"])   # Cliente malicioso

degradacao = (acc_benign - acc_malign) / acc_benign * 100
if degradacao > 5:  # Threshold de 5%
    print("‚ö†Ô∏è  Cliente suspeito detectado!")
```

### 4.3 Manipula√ß√£o de Dados (Data Poisoning)

Implementa√ß√£o do ataque de envenenamento para simula√ß√£o real√≠stica de amea√ßas.

#### 4.3.1 Ataque Label Flipping

**Estrat√©gia**: Modifica√ß√£o sistem√°tica de labels espec√≠ficas

```python
def manipule_data():
    """Implementa ataque 7‚Üí1 no dataset MNIST"""
    
    # Carrega dataset original
    mnist_trainset = datasets.MNIST(root=svar.PATH_BASE_DATASET.value, 
                                   train=True, download=False, 
                                   transform=netTransform)
    
    # Aplica poisoning: todas as labels 7 viram 1
    for i in range(len(mnist_trainset)):
        if mnist_trainset.targets[i] == 7:
            mnist_trainset.targets[i] = 1
    
    # Divide e salva dataset modificado
    train_dataset, test_dataset = random_split(mnist_trainset, [train_size, test_size])
    torch.save((train_data, train_targets), 
               './datasets/poisoned_data_set_1/training.pt')
```

#### 4.3.2 Caracter√≠sticas do Ataque

**Por que 7‚Üí1 √© efetivo:**
- **Similaridade visual**: D√≠gitos 7 e 1 compartilham caracter√≠sticas (linhas verticais)
- **Sutileza**: N√£o degrada drasticamente performance geral
- **Direcionamento**: Afeta especificamente uma classe (stealth attack)

**Impacto esperado:**
- Acur√°cia geral: Redu√ß√£o moderada (5-15%)
- Classe 7: Degrada√ß√£o severa (>80% classificados como 1)
- Outras classes: Performance mantida (camuflagem)

---

## 5. Conceitos Fundamentais do PyTorch

### 5.1 Camadas Convolucionais (Conv2d)

**Defini√ß√£o**: Opera√ß√£o fundamental em vis√£o computacional que aplica filtros deslizantes sobre a imagem para detectar caracter√≠sticas locais.

**Funcionamento**:
```python
nn.Conv2d(in_channels, out_channels, kernel_size)
```

**Par√¢metros principais**:
- `in_channels`: N√∫mero de canais de entrada (1 para grayscale, 3 para RGB)
- `out_channels`: N√∫mero de filtros/caracter√≠sticas detectadas
- `kernel_size`: Tamanho da janela deslizante (ex: 3√ó3, 5√ó5)

**Como funciona**:
1. Um filtro (kernel) desliza sobre a imagem
2. Em cada posi√ß√£o, calcula produto escalar entre filtro e regi√£o da imagem
3. Gera um mapa de ativa√ß√£o mostrando onde a caracter√≠stica foi detectada

**Vantagens**:
- **Compartilhamento de par√¢metros**: Mesmo filtro usado em toda imagem
- **Invari√¢ncia espacial**: Detecta caracter√≠sticas independente da posi√ß√£o
- **Hierarquia**: Camadas profundas detectam padr√µes mais complexos

### 5.2 Regulariza√ß√£o com Dropout

**Defini√ß√£o**: T√©cnica que aleatoriamente "desliga" neur√¥nios durante treinamento para prevenir overfitting.

**Tipos no c√≥digo**:

#### Dropout2d
```python
nn.Dropout2d(p=0.5)  # Zera 50% dos canais completos
```
- Aplicado em camadas convolucionais
- Remove mapas de caracter√≠sticas inteiros
- Mais efetivo que dropout pixel-wise em CNNs

#### Dropout1d (nas camadas densas)
```python
F.dropout(x, training=self.training)
```
- Aplicado em camadas lineares
- Remove neur√¥nios individuais
- Apenas ativo durante `training=True`

**Por que funciona**:
1. **Redund√¢ncia for√ßada**: Rede n√£o pode depender de poucos neur√¥nios
2. **Generaliza√ß√£o**: Melhora performance em dados n√£o vistos
3. **Ensemble impl√≠cito**: Simula m√∫ltiplas redes menores

**Analogia**: Como estudar sem depender sempre do mesmo colega - for√ßa aprendizado independente.

### 5.3 Camadas Lineares (Linear)

**Defini√ß√£o**: Transforma√ß√£o linear b√°sica (multiplica√ß√£o matriz + bias) usada para classifica√ß√£o final.

```python
nn.Linear(in_features, out_features)
```

**Opera√ß√£o matem√°tica**:
```
y = xW^T + b
onde:
- x: entrada (batch_size, in_features)
- W: pesos trein√°veis (out_features, in_features)
- b: bias (out_features)
```

**Uso na arquitetura**:
- **fc1**: Combina caracter√≠sticas extra√≠das pelas convolucionais
- **fc2**: Mapeia para n√∫mero de classes (10 d√≠gitos)

**Transi√ß√£o espacial‚Üíconceitual**:
- Convolucionais: trabalham com informa√ß√£o espacial (2D)
- Lineares: trabalham com representa√ß√µes abstratas (1D)

### 5.4 Fun√ß√µes de Ativa√ß√£o

#### 5.4.1 ReLU

**Defini√ß√£o**: Rectified Linear Unit - fun√ß√£o de ativa√ß√£o n√£o-linear simples e eficaz.

**F√≥rmula**: `f(x) = max(0, x)`

**Comportamento**:
```
x < 0: f(x) = 0     (zera valores negativos)
x ‚â• 0: f(x) = x     (mant√©m valores positivos)
```

**Vantagens**:
- **Computacionalmente eficiente**: Opera√ß√£o simples de compara√ß√£o
- **Gradientes limpos**: Evita problema de desaparecimento de gradiente
- **Esparsidade**: Muitos neur√¥nios ficam inativos (zero)

**Por que √© importante**:
- Sem ativa√ß√£o n√£o-linear, rede seria apenas regress√£o linear
- Permite aprender padr√µes complexos e n√£o-lineares
- Introduz capacidade de "decis√£o" (ativa ou n√£o ativa)

#### 5.4.2 Softmax e LogSoftmax

**Softmax**:
- Converte valores brutos em probabilidades que somam 1
- `softmax(x_i) = exp(x_i) / Œ£ exp(x_j)`

**LogSoftmax**:
- Logaritmo natural do softmax
- `log_softmax(x_i) = x_i - log(Œ£ exp(x_j))`

**Por que LogSoftmax**:
- **Estabilidade num√©rica**: Evita underflow/overflow
- **Efici√™ncia**: Melhor para fun√ß√£o de perda NLLLoss
- **Gradientes**: Computa√ß√£o mais est√°vel durante backpropagation

### 5.5 Max Pooling

**Defini√ß√£o**: Opera√ß√£o de downsampling que reduz dimens√µes espaciais mantendo informa√ß√µes mais importantes.

**Funcionamento**:
```python
F.max_pool2d(x, kernel_size=2)
```

**Processo**:
1. Divide imagem em janelas n√£o-sobrepostas (ex: 2√ó2)
2. Toma valor m√°ximo de cada janela
3. Reduz dimens√£o pela metade

**Exemplo visual**:
```
Entrada 4√ó4:          Sa√≠da 2√ó2 (ap√≥s max_pool 2√ó2):
[1  3  2  4]          [7  8]
[5  7  6  8]    ‚Üí     [15 16]
[9  11 10 12]
[13 15 14 16]
```

**Benef√≠cios**:
- **Redu√ß√£o computacional**: Menos par√¢metros nas camadas seguintes  
- **Invari√¢ncia √† transla√ß√£o**: Pequenos deslocamentos n√£o afetam resultado
- **Abstra√ß√£o**: Foca nas caracter√≠sticas mais "fortes"
- **Controle de overfitting**: Reduz complexidade do modelo

### 5.6 Normaliza√ß√£o de Dados

**No contexto MNIST**:
```python
transforms.Normalize((0.1307,), (0.3081,))
```

**F√≥rmula**: `x_normalizado = (x - m√©dia) / desvio_padr√£o`

**Por que normalizar**:
- **Converg√™ncia mais r√°pida**: Gradientes mais est√°veis
- **Escala uniforme**: Todas as features t√™m mesma import√¢ncia inicial
- **Estabilidade num√©rica**: Evita valores muito grandes/pequenos

**Valores MNIST**:
- `m√©dia = 0.1307`: Valor m√©dio dos pixels no dataset
- `desvio = 0.3081`: Dispers√£o padr√£o dos pixels
- **Resultado**: Pixels normalizados entre aproximadamente [-0.4, 2.8]

**Pipeline completo de pr√©-processamento**:
```python
transforms.Compose([
    transforms.Grayscale(num_output_channels=1),  # Garante 1 canal
    transforms.Resize((28, 28)),                  # Padroniza tamanho
    transforms.ToTensor(),                        # [0,255] ‚Üí [0,1]
    transforms.Normalize((0.1307,), (0.3081,))    # Normaliza√ß√£o final
])
```

---

## 6. Funcionalidades Avan√ßadas (Em Desenvolvimento)

O sistema possui infraestrutura preparada para t√©cnicas avan√ßadas de explicabilidade e an√°lise:

### 6.1 LIME (Local Interpretable Model-Agnostic Explanations)

**Objetivo**: Entender quais pixels/regi√µes influenciam decis√µes do modelo para detectar vieses introduzidos por poisoning.

```python
def lime():
    """Interface preparada para an√°lise LIME"""
    # M√©todo predict() na classe Net j√° est√° otimizado para LIME
    # - Aceita arrays numpy 
    # - Aplica normaliza√ß√£o automaticamente
    # - Retorna probabilidades (n√£o log-probs)
```

**Aplica√ß√£o na detec√ß√£o:**
- **Modelo benigno**: Foca em caracter√≠sticas normais dos d√≠gitos
- **Modelo envenenado**: Pode mostrar depend√™ncia an√¥mala de pixels espec√≠ficos que diferenciam 7 de 1

### 6.2 Grad-CAM (Gradient-weighted Class Activation Mapping)

**Objetivo**: Visualizar mapas de aten√ß√£o para identificar regi√µes que o modelo considera mais importantes.

```python
def gradCAM():
    """Visualiza√ß√£o de mapas de ativa√ß√£o por gradientes"""
    # Implementa√ß√£o futura para an√°lise de attention maps
    # √ötil para verificar se modelo envenenado foca em regi√µes suspeitas
```

### 6.3 An√°lises Gr√°ficas Avan√ßadas

```python
def get_graphics():
    """Gera√ß√£o de visualiza√ß√µes a partir de dados CSV salvos"""
    # Dashboard para monitoramento cont√≠nuo
    # Gr√°ficos de evolu√ß√£o de acur√°cia ao longo do tempo
    # Heatmaps de similaridade entre modelos de clientes
```

### 6.4 Sistema de CSV para An√°lise Longitudinal

Estrutura preparada para logging e an√°lise temporal:

```python
# Tabela de an√°lises (analyses.csv)
train_labels = {
    "train_id": [],         # Identificador √∫nico do experimento
    "accuracy": [],         # Acur√°cia global
    "benign_clients": [],   # N√∫mero de clientes benignos
    "malignant_clients": [], # N√∫mero de clientes maliciosos  
    "poisoning": []         # Tipo de ataque aplicado
}

# Matriz de confus√£o (map.csv) 
map_labels = {
    "train_id": [],
    "0": [], "1": [], ..., "9": []  # Predi√ß√µes para cada classe
}
```

---

## 7. Fluxo Operacional Completo

### Execu√ß√£o do Sistema

**1. Configura√ß√£o inicial:**
```bash
python app/dashboard.py
```

**2. Pipeline autom√°tico:**
```python
def main():
    # Opcional: Treinar clientes (benigno + malicioso)
    # post_client_train()
    
    # Executado por padr√£o: Gerar dataset envenenado
    manipule_data()
```

**3. An√°lise manual (via controler):**
```python
# Treinar clientes espec√≠ficos
post_client_train()

# Agregar modelos selecionados  
post_central_train(selected_indice_models=[4, 5])

# Executar an√°lises comparativas
get_analyses()
```

### Interpreta√ß√£o de Resultados

**Indicadores de poisoning detectados:**

1. **Degrada√ß√£o de acur√°cia**: >5% comparado ao baseline benigno
2. **Matriz de confus√£o assim√©trica**: Alto 7‚Üí1, baixo 1‚Üí7  
3. **Performance seletiva**: Outras classes mant√™m acur√°cia normal

**Exemplo de output esperado:**
```
Acc benign client model 4:  0.9823  (98.23%)
Acc malign client model 5:  0.9156  (91.56%)

üö® Degrada√ß√£o detectada: 6.79%
üîç An√°lise da matriz de confus√£o recomendada
```

### Extensibilidade

O sistema foi projetado com expansibilidade em mente:

- **Novos tipos de ataque**: Facilmente implement√°veis em `manipule_data()`
- **Algoritmos de agrega√ß√£o**: M√≥dulo `central/` aceita diferentes estrat√©gias
- **M√©tricas customizadas**: Sistema de an√°lises modular
- **Datasets alternativos**: Suporte via modifica√ß√£o de `sysvars.py`

---

## Conclus√£o

Este sistema fornece uma **base s√≥lida e extens√≠vel** para pesquisa em seguran√ßa de aprendizado federado, combinando:

‚úÖ **Simula√ß√£o real√≠stica** de cen√°rios federados  
‚úÖ **Implementa√ß√£o de ataques** conhecidos (label flipping)  
‚úÖ **Ferramentas de detec√ß√£o** baseadas em m√©tricas estat√≠sticas  
‚úÖ **An√°lise explic√°vel** com infraestrutura para LIME/Grad-CAM  
‚úÖ **Arquitetura modular** para f√°cil extens√£o  

O projeto demonstra como **data poisoning** pode degradar modelos de forma sutil e fornece m√©todos pr√°ticos para detectar tais amea√ßas em ambientes distribu√≠dos reais.
